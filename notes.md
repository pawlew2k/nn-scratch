# Notes

## Q&A

1. Łatwa konfiguracja ... obecności biasów -> czy chodzi po prostu o obecność, czy w jakiś sposób
   konfigurację początkową biasów
2. wizualizacja błędu propagowanego w kolejnych iteracjach uczenia (na każdej z wag) - czy chodzi o
   MSE (jak wygląda los w kolejnych iteracjach), czy o coś innego?
3. kilka różnych architektur -> czy chodzi jedynie o ilość warstw i liczebność perceptronów w
   warstwach
4. drop rate - czy powinniśmy używać, aby zapobiegać overfittingowi?
5. czy można używać gotowych metryk, czy trzeba je implementować samemu?

## Gradient descent

https://medium.com/@sami.benbrahim/gradient-descent-from-scratch-an-overview-of-gd-variants-f558da269a5f

- we have chosen stochastic (SGD) for 1st task